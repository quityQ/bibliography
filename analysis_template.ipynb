{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following analysis is based on the following search query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyBibX.base import pbx_probe\n",
    "from tabulate import tabulate\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bib file\n",
    "# Arguments for pbx_probe: file_bib = 'filename.bib'; db = 'scopus', 'wos', 'pubmed'; del_duplicated = True, False\n",
    "# Note: the other arguments lead to an key error for some reason \n",
    "\n",
    "file_path = ## path to the bib file\n",
    "database = 'scopus'\n",
    "bibfile = pbx_probe(file_bib = file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic report\n",
    "report = bibfile.eda_bib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bibfile.data is the dataframe with the content\n",
    "print(tabulate(bibfile.data.head(n = 10), headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Docs IDs\n",
    "print(tabulate(bibfile.table_id_doc, headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DocIDs per Type\n",
    "print(tabulate(bibfile.id_doc_types(), headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check authors\n",
    "print(tabulate(bibfile.table_id_aut, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sources/journals\n",
    "print(tabulate(bibfile.table_id_jou, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check institutions\n",
    "print(tabulate(bibfile.table_id_uni, headers='keys', tablefmt='psql'))\n",
    "\n",
    "# Note: maybe there is an option to import institution on scopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check countries\n",
    "print(tabulate(bibfile.table_id_ctr, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check authors keyword (kwa)\n",
    "print(tabulate(bibfile.table_id_kwa, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check keyword plus IDs\n",
    "print(tabulate(bibfile.table_id_kwp, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud from abstracts\n",
    "#Other arguments for entry      = 'abs', 'title', 'kwa', or 'kwp'\n",
    "bibfile.word_cloud_plot(entry='abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting words in abstracts by importance\n",
    "table             = PrettyTable()\n",
    "data_wd           = bibfile.ask_gpt_wd\n",
    "table.field_names = ['Word', 'Importance']\n",
    "for key, value in data_wd.items():\n",
    "    table.add_row([key, round(value, 4)])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Grams\n",
    "# Arguments: view       = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
    "#            entry      = 'abs', 'title', 'kwa', or 'kwp'\n",
    "#            n_grams    = An integer with size n (representing the most common groups of words with size n)\n",
    "#            stop_words = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
    "#                         'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
    "#                         'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Portuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
    "#                         'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
    "#            rmv_custom_words  = A list of custom stopwords to clean the corpus\n",
    "#             wordsn           = Number of N-Grams\n",
    "# Arguments can be changed to fit the user's needs\n",
    "bibfile.get_top_ngrams(view = 'notebook', entry = 'abs', ngrams = 2, stop_words = [], rmv_custom_words = [], wordsn = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of the most common n-grams\n",
    "data_ng = bibfile.ask_gpt_ng\n",
    "print(data_ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents Projection based on Words. (An interactive plot). It returns the Projection (each document coordinate) and the Labels (each document cluster)\n",
    "# Arguments: view              = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
    "#            corpus_type       = 'abs', 'title', 'kwa', or 'kwp';\n",
    "#            stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
    "#                                'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
    "#                                'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
    "#                                'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
    "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
    "#            custom_label      = A list of custom labels for each document. The user can define each document cluster;\n",
    "#            custom_projection = A list of custom coordinates for each document. The user can define each document coordinate;\n",
    "#            n_components      = Number of Dimensions;\n",
    "#            n_clusters        = Number of Clusters;\n",
    "#            tf_idf            = True or False (True -> The Cluster Algorithm will use the DTM to calculate each document Label. False -> The Cluster Algorithm will use the Coordinates to calculate each document Label)\n",
    "#            embeddings        = True or False (True -> The Cluster Algorithm will use the Word Embeddings to calculate each document Label. False -> The Cluster Algorithm will use the Coordinates to calculate each document Label)\n",
    "#            method            = 'tsvd' or 'umap' ('tsvd' -> Truncated SVD projection method is used. 'umap' -> UMAP projection method is used)\n",
    "# Arguments can be changed to fit the user's needs\n",
    "projection, labels = bibfile.docs_projection(view              = 'notebook',\n",
    "                                             corpus_type       = 'abs',\n",
    "                                             stop_words        = ['en'],\n",
    "                                             rmv_custom_words  = [],\n",
    "                                             custom_label      = [],\n",
    "                                             custom_projection = [],\n",
    "                                             n_components      = 2,\n",
    "                                             n_clusters        = 5,\n",
    "                                             tf_idf            = False,\n",
    "                                             embeddings        = False,\n",
    "                                             method            = 'umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of the above\n",
    "data_pr = pd.DataFrame(np.hstack([projection, labels.reshape(-1,1)]))\n",
    "print(tabulate(data_pr, headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments: view              = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
    "#            key               = 'abs', 'title', 'jou, 'kwa', or 'kwp';\n",
    "#            stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
    "#                                'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
    "#                                'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
    "#                                'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
    "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
    "#            topn              = Total number entities;\n",
    "#            txt_font_size     = Font size of the text inside the bins;\n",
    "#            start             = Start Year; -1 = all years\n",
    "#            end               = End Year;   -1 = all years\n",
    "# Arguments can be changed to fit the user's needs\n",
    "bibfile.plot_evolution_year(view             = 'notebook',\n",
    "                            stop_words       = ['en'],\n",
    "                            rmv_custom_words = [],\n",
    "                            key              = 'abs',\n",
    "                            topn             = 10,\n",
    "                            txt_font_size    = 12,\n",
    "                            start            = 2014,\n",
    "                            end              = 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sankey Diagram (An interactive plot)\n",
    "# Arguments: view  = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
    "#            entry = a list of any length of the following keys -> 'aut', 'cout', 'inst', 'jou', 'kwa', 'kwp', 'lan';\n",
    "#            topn  = Total number entities\n",
    "# Arguments can be changed to fit the user's needs\n",
    "bibfile.sankey_diagram(view = 'notebook', entry = ['aut', 'cout', 'inst', 'lan'], topn = 10)\n",
    "\n",
    "# The white bars can be dragged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of sankey diagram\n",
    "data_sk = bibfile.ask_gpt_sk\n",
    "print(tabulate(data_sk, headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Map\n",
    "# Arguments: entry         = 'kwp', 'kwa', 'aut', 'jou', 'ctr', or 'inst';\n",
    "#            topn          = Total number entities\n",
    "#            txt_font_size = Font size of the text inside the bins;\n",
    "#Arguments can be changed to fit the user's needs\n",
    "bibfile.tree_map(entry = 'jou', topn = 20, size_x = 30, size_y = 10, txt_font_size = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors Productivity Plot (An interactive plot). It informs for each year the documents (IDs) published for each author\n",
    "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
    "#            topn = Total number entities\n",
    "# Arguments can be changed to fit the user's needs\n",
    "bibfile.authors_productivity(view = 'notebook', topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Plots\n",
    "# Arguments: statistic = 'dpy', 'cpy', 'ppy', 'ltk', 'spd', 'spc', 'apd', 'apc', 'aph', 'bdf_1', 'bdf_2', 'bdf_3', 'ipd', 'ipc', 'cpd', 'cpc', 'lpd', 'kpd', 'kad'\n",
    "#                        'dpy' = Documents per Year\n",
    "#                         cpy' = Citations per Year\n",
    "#                        'ppy' = Past Citations per Year\n",
    "#                        'ltk' = Lotka's Law\n",
    "#                        'spd' = Sources per Documents\n",
    "#                        'spc' = Sources per Citations\n",
    "#                        'apd' = Authors per Documents\n",
    "#                        'apc' = Authors per Citations\n",
    "#                        'aph' = Authors per H-Index\n",
    "#                        'bdf_1', 'bdf_2', 'bdf_3' = Bradford's Law - Core Sources 1, 2 or 3\n",
    "#                        'ipd' = Institutions per Documents\n",
    "#                        'ipc' = Institutions per Citations\n",
    "#                        'cpd' = Countries per Documents\n",
    "#                        'cpc' = Countries per Citations\n",
    "#                        'lpd' = Language per Documents\n",
    "#                        'kpd' = Keywords Plus per Documents\n",
    "#                        'kad' = Authors' Keywords per Documents\n",
    "#            topn      = Total number entities\n",
    "# Arguments can be changed to fit the user's needs\n",
    "bibfile.plot_bars(statistic = 'apd', topn = 20, size_x = 15, size_y = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table of the above\n",
    "data_bp = bibfile.ask_gpt_bp\n",
    "print(tabulate(data_bp, tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network - Citation Analisys Between Documents (Blue Nodes) and Citations (Red Nodes).  (An interactive plot).\n",
    "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
    "#            min_count   = Relationship between nodes that have been cited at least x times;\n",
    "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
    "#            local_nodes = True or False (True -> Only the blue will be displayed, False -> Red and Blue nodes will be displayed)\n",
    "# Arguments can be changed to fit the user's needs\n",
    "bibfile.network_adj_dir(view = 'notebook', min_count = 0, node_labels = True, local_nodes = False)\n",
    "\n",
    "# Note: in the regular bibtex export there is no detailed information about the citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network - Local Documents (Only Blue Nodes) Citation History. (An interactive plot).\n",
    "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
    "#            min_count   = Relationship between nodes that have connected at least x times;\n",
    "#            node_size   = -1. (If node_size = -1 then the default value will be used. If node_size > 0 then this new value will be used);\n",
    "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
    "#            back        = A list of documents. It shows the documents cited by them direct and indirectly;\n",
    "#            forward     = A list of documents. It shows the documents that cites them direct and indirectly\n",
    "# Arguments can be changed to fit the user's needs\n",
    "bibfile.network_hist(view = 'notebook', min_count = 0, node_size = -1, node_labels = True, back = [], forward = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network - Collaboration Analysis Between Authors, Countries, Intitutions Or Adjacency Analysis Between Authors' Keywords or Keywords Plus. (An interactive plot).\n",
    "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
    "#            adj_type    = 'aut', 'cout', 'inst', 'kwa', or 'kwp'\n",
    "#            min_count   = Relationship between nodes that have connected at least x times;\n",
    "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
    "#            node_size   = -1. (If node_size = -1 then the default value will be used. If node_size > 0 then this new value will be used);\n",
    "#            label_type  = 'id', 'name' (Only meaningfull if node_labels = True. 'id' -> The ID will be displayed; 'name' -> The name will be displayed);\n",
    "#            centrality  = 'degree', 'load', 'betw', 'close', 'eigen', 'katz', 'harmonic', or None. Color nodes according to centrality criterion\n",
    "#                          'degree'   = Degree Centrality\n",
    "#                          'load'     = Load Centrality\n",
    "#                          'betw'     = Betweenness Centrality\n",
    "#                          'close'    = Closeness Centrality\n",
    "#                          'eigen'    = Eigenvector Centrality\n",
    "#                          'katz'     = Katz Centrality\n",
    "#                          'harmonic' = Harmonic Centrality\n",
    "#                           None      = The Community Algorithm, Girvan-Newman, will be used Instead of a Centrality Criterion\n",
    "# Arguments can be changed to fit the user's needs\n",
    "bibfile.network_adj(view = 'notebook', adj_type = 'aut', min_count = 5, node_labels = True, label_type = 'name', centrality = None)\n",
    "\n",
    "# PS: If a centrality criterion is used then the values can be obtained by the following command:  bibfile.table_centr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network - Similarity Analysis using coupling or cocitation methods. (An interactive plot).\n",
    "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
    "#            sim_type    = 'coup', 'cocit' ('coup' -> Coupling Method, 'cocit' -> Cocitation Method)\n",
    "#            node_size   = -1. (If node_size = -1 then the default value will be used. If node_size > 0 then this new value will be used);\n",
    "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
    "#            cut_coup    = Cutoff value for Coupling Method. Only meaninfull if sim_type = 'coup';\n",
    "#            cut_cocit   = Cutoff value for Cocitation Method. Only meaninfull if sim_type = 'cocit'\n",
    "# Arguments can be changed to fit the user's needs\n",
    "bibfile.network_sim(view = 'notebook', sim_type = 'cocit', node_size = -1, node_labels = True, cut_coup = 0.3, cut_cocit = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network - Collaboration Analysis Between Countries using a Map. (An interactive plot).\n",
    "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
    "#            connections = True or False (True -> Countries connections will be displayed, False -> Countries connections will not be displayed);\n",
    "#            country_lst = Highlight the Connections Between a List of Countries\n",
    "# Arguments can be changed to fit the user's needs\n",
    "bibfile.network_adj_map(view = 'notebook', connections = True, country_lst = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "# Arguments: corpus_type       = 'abs', 'title', 'kwa', or 'kwp';\n",
    "#            stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
    "#                                'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
    "#                                'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
    "#                                'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
    "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
    "# Arguments can be changed to fit the user's needs\n",
    "bibfile.create_embeddings(stop_words = ['en'], rmv_custom_words = [], corpus_type = 'abs')\n",
    "emb = bibfile.embds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP #-1 refers to all outliers and should typically be ignored.\n",
    "# Arguments: stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
    "#                              'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
    "#                              'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
    "#                              'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainianian;   'es' =  Spanish;  'sv' = Swedish\n",
    "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
    "#            embeddings        = True or False. If True then word embeddings are used to create the topics\n",
    "# Arguments can be changed to fit the user's needs\n",
    "bibfile.topics_creation(stop_words = ['en'], rmv_custom_words = [], embeddings = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "# Each document Topic\n",
    "topics = bibfile.topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "# Each document Probability to belong a Topic\n",
    "probs = bibfile.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
    "bibfile.graph_topics_distribution(view = 'notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
    "bibfile.graph_topics(view = 'notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "bibfile.topics_representatives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "similar_topics, similarity = bibfile.topic_model.find_topics('electre', top_n = 10)\n",
    "for i in range(0, len(similar_topics)):\n",
    "  print('Topic: ', similar_topics[i], 'Correlation: ', round(similarity[i], 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
